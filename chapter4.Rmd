# Exercise 4: Clustering and classification

*November 22, 2019*  
*This week I worked with Boston dataset.*  

### {.tabset}

#### **Playing with the dataset**  

**Loading** the Boston data and exploring the structure and dimensions: 
```{r}
library(MASS)
data('Boston')
str(Boston)
dim(Boston)
```
This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass.  
It has 506 observations (rows) and 14 variables (columns).  

**Graphical overview** of the data:
```{r}
pairs(Boston)
```

We can see scatterplots of the relationships of data's variables.  
What we are looking here are any linear or U-shaped patterns, that might tell us about some dependancies. For instance, we can see some correlation patterns between "nox" and "dis", "lstat" and "medv", "lstat" and "rm", "medv" and "rm".  
*nox - nitric oxides concentration*  
*dis - weighted distances to five Boston employment centres*  
*lstat - % lower status of the population*  
*medv - median value of owner-occupied homes in $1000's*  
*rm - average number of rooms per dwelling*  

**Summaries** of the variables:
```{r}
summary(Boston)
```

We can see that the "chas" variable looks very weird, but it is due to it being integer (0 and 1).  
Also, the "black" variable clearly has an outlier.  
*chas - Charles River dummy variable (1 if tract bounds river; 0 otherwise)*  
*black - proportion of blacks by town*  

**Scaling** the dataset:  
```{r}
boston_scaled <- scale(Boston)
summary(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled) #making our object a dataframe
```

The variables changed. We subtracted the column means from the corresponding columns and divided the difference with standard deviation. Now all variables means are zeros.  

**Creating a categorical variable:**
```{r}
bins <- quantile(boston_scaled$crim) #using the quantiles as the break points
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high")) #creating a variable
boston_scaled <- dplyr::select(boston_scaled, -crim) #dropping the old crime variable
boston_scaled <- data.frame(boston_scaled, crime) #adding the new crime variable to the dataset
```

Now lets divide the dataset to **train** and **test** ones:
```{r}
n <- nrow(boston_scaled) #number of rows
ind <- sample(n,  size = n * 0.8) #random choice of 80% of rows
train <- boston_scaled[ind,] #train set
test <- boston_scaled[-ind,] #test set
```

#### **Linear Discriminant Analysis**  

Lets **fit the LDA** on the train set:
```{r}
lda.fit <- lda(crime ~ ., data = train) #crime is a target variable and all others are predictors
```

**(Bi)Plotting the LDA**:
```{r}
classes <- as.numeric(train$crime) #we are creating a numeric vector of the train sets crime classes for plotting purposes
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1) #plotting the LDA
```

Now lets do some changes:
```{r}
correct_classes <- test$crime #saving the correct classes from test data
test <- dplyr::select(test, -crime) #removing the crime variable from test data
```

We can now **predict the classes** on the test data:
```{r}
lda.pred <- predict(lda.fit, newdata = test)
```

We can also **cross-tabulate** the results:
```{r}
table(correct = correct_classes, predicted = lda.pred$class)
```

I don't think the LDA model predicted the classes well. It made some major mistakes regarding med_low and med_high predictions.  

#### **K-means and other stuff**  

For a start:
```{r}
library(MASS)
data('Boston')
Boston2 <- scale(Boston) #standartizing (scaling) a dataset
Boston2 <- as.data.frame(Boston2) #saving as dataframe
dist_eu <- dist(Boston2) #calculating distances between observations
```

Now lets run k-means:
```{r}
km <- kmeans(x = Boston2, centers = 3)
```

Now we can determine the optimal number of clusters:
```{r}
set.seed(123) #this function prevents k-means from randomly assigning the initial cluster centers
k_max <- 10 #let the maximum amount of clusters be 10
twcss <- sapply(1:k_max, function(k){kmeans(Boston2, k)$tot.withinss}) #calculating the total sum of squares
library(ggplot2)
qplot(x = 1:k_max, y = twcss, geom = 'line') #visualizing the clusters
```

The total WCSS changes radically around 2, which means **2 is an optimal number of clusters.**  
Lets run the code again and plot it:
```{r}
km <-kmeans(Boston2, centers = 2)
pairs(Boston2, col = km$cluster)
```

